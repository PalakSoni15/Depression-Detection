# -*- coding: utf-8 -*-
"""FinalYearPBL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMpfPcx9-SkcHuzsVsyLhR1QehOUOapc

###Import required dependencies
"""

import warnings

import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
import csv
import os
import sklearn
from math import exp
from numpy import sign

from tensorflow import keras
from gensim.models import KeyedVectors
from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score
from keras.models import Model, Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.callbacks import LearningRateScheduler

"""###Import Preprocessed tweets"""

def getTexts():
  PositiveText = []
  NegativeText = []
  f1 = open("D:\\Final Project\\backend_palak\\main\\PositiveTweets.csv.xls", 'r')
  file1 = csv.DictReader(f1)
  for col in file1:
    PositiveText.append(col['text'])

  f2 = open("D:\\Final Project\\backend_palak\\main\\NegativeTweets.csv.xls", 'r')
  file2 = csv.DictReader(f2)
  for col in file2:
    NegativeText.append(col['text'])
  
  return PositiveText, NegativeText

"""###Import vectorizer"""

def get_word2vec():
  EMBEDDING_FILE = "D:\\Final Project\\backend_palak\\main\\GoogleNews-vectors-negative300.bin"
  word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)
  return word2vec

"""###TOKENIZER"""

def tokenize(PositiveText, NegativeText):
  tokenizer = Tokenizer(num_words=10000)
  tokenizer.fit_on_texts(PositiveText + NegativeText)

  sequences_happy = tokenizer.texts_to_sequences(PositiveText)
  sequences_sad = tokenizer.texts_to_sequences(NegativeText)

  word_index = tokenizer.word_index

  data_happy = pad_sequences(sequences_happy, maxlen=36, truncating='post', padding='post')
  data_sad = pad_sequences(sequences_sad, maxlen=36, truncating='post', padding='post')

  return tokenizer, data_happy, data_sad, word_index


"""###EMBEDDING MATRIX"""

def embeddingMatrix(word_index, word2vec):
  nb_words = min(10000, len(word_index))

  embedding_matrix = np.zeros((nb_words, 300))

  for (word, idx) in word_index.items():
      if word in word2vec and idx < 10000:
          embedding_matrix[idx] = word2vec.word_vec(word)
  return embedding_matrix

#embedding_matrix.shape

"""###Train, Test, Validation Split"""

def split(data_sad, data_happy):
  # Assigning labels to the depressive tweets and random tweets data
  labels_happy = np.array([0] * 315929)
  labels_sad = np.array([1] * 317232)

  # Splitting the arrays into train data (60%), validation (20%), and testing (20%)
  perm_sad = np.random.permutation(len(data_sad))
  idx_train_sad = perm_sad[:int(len(data_sad)*(0.6))]
  idx_test_sad = perm_sad[int(len(data_sad)*(0.6)):int(len(data_sad)*(0.6+0.2))]
  idx_val_sad = perm_sad[int(len(data_sad)*(0.6+0.2)):]

  perm_happy = np.random.permutation(len(data_happy))
  idx_train_happy = perm_happy[:int(len(data_happy)*(0.6))]
  idx_test_happy = perm_happy[int(len(data_happy)*(0.6)):int(len(data_happy)*(0.6+0.2))]
  idx_val_happy = perm_happy[int(len(data_happy)*(0.6+0.2)):]

  # Combine depressive tweets and random tweets arrays
  data_train = np.concatenate((data_sad[idx_train_sad], data_happy[idx_train_happy]))
  labels_train = np.concatenate((labels_sad[idx_train_sad], labels_happy[idx_train_happy]))
  data_test = np.concatenate((data_sad[idx_test_sad], data_happy[idx_test_happy]))
  labels_test = np.concatenate((labels_sad[idx_test_sad], labels_happy[idx_test_happy]))
  data_val = np.concatenate((data_sad[idx_val_sad], data_happy[idx_val_happy]))
  labels_val = np.concatenate((labels_sad[idx_val_sad], labels_happy[idx_val_happy]))

  # Shuffling
  perm_train = np.random.permutation(len(data_train))
  data_train = data_train[perm_train]
  labels_train = labels_train[perm_train]
  perm_test = np.random.permutation(len(data_test))
  data_test = data_test[perm_test]
  labels_test = labels_test[perm_test]
  perm_val = np.random.permutation(len(data_val))
  data_val = data_val[perm_val]
  labels_val = labels_val[perm_val]

  return data_train, labels_train, data_test, labels_test, data_val, labels_val

"""###----"""

def get_data():
  PositiveText, NegativeText = getTexts()
  word2vec = get_word2vec()
  tokenizer, data_happy, data_sad, word_index = tokenize(PositiveText, NegativeText)
  embedding_matrix = embeddingMatrix(word_index, word2vec)
  data_train, labels_train, data_test, labels_test, data_val, labels_val = split(data_sad, data_happy)

  return tokenizer, embedding_matrix, data_train, labels_train, data_test, labels_test, data_val, labels_val

tokenizer, embedding_matrix, data_train, labels_train, data_test, labels_test, data_val, labels_val = get_data()

def CreateModel():
  model = Sequential()
  # Embedded layer
  model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=36, trainable=True))
  # Convolutional Layer
  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
  model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))
  model.add(Dropout(0.1))
  # LSTM Layer
  model.add(LSTM(300))
  model.add(Dropout(0.1))
  model.add(Dense(1, activation='sigmoid'))

  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])
  print(model.summary())

  return model

model = CreateModel()

"""#Model Training"""
"""
checkpoint_path = "Checkpoints/training_1/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

lr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))

#create checkpoint callback
model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

# early_stop = EarlyStopping(monitor='val_loss', patience=15)
callbacks = [lr_sched, model_checkpoint_callback]

hist = model.fit(data_train, labels_train, \
        validation_data=(data_val, labels_val), \
        epochs=30, batch_size=400, shuffle=True, \
        callbacks=callbacks)

print("DONE")

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

path = F"/content/drive/My Drive/models/pbl_model3.htf5" 
model.save(path)
"""
"""#Model Testing"""

model = keras.models.load_model("D:\\Final Project\\backend_palak\\main\\pbl_model3.htf5")

loss, acc = model.evaluate(data_val, labels_val)
print('model validation accurary = : {:5.2f}%'.format(100*acc))

testing_text = {'How are you', 
                'Life is short to be depressed ',
                'Find joy in little things of life',
                "I really hate this life I'm living I don't even understand why we are alive ",
                "Enjoy you life, be grateful for every minute you spend with you lovedones, I really love my life.",
                "My tag line of life is 'Biti Baton pe dhul udata chala' a line from my favourite singer Kishore Kumar which Means just 'Forget the past' so I always think about future, no depression.",
                "Life is beautiful",
                "Hi my freinds i hope you are doing well i wanted just to say hello to you in this beautifull journy have a nice day ",        
                "Maybe nothing in this life happens by accident. As everything happens for a reason, our destiny slowly takes form...",
                "I sometimes wants to end my life though I know that everything will be okay soon ",
                "I can do better if I really want to do something.",
                "Spending a lot of your time scared and anxious is exhausting.  I was lucky to meet some amazing women in my thirties who made me realise that all the stuff I've been paranoid about since I was a teenager was universal.  Weight, skin, feminism and pubes, all open topics of discussion that made me wish I had known women like them in my teenage years.",
                "I think life is just spending time with a really bd depression and dying at the end",
                "I can overthink it because I'm already overthinking my overthinking thank you depression"}

def input_text(text):
  text_tokened = tokenizer.texts_to_sequences([text])
  text_padded = pad_sequences(text_tokened, maxlen=36, truncating='post', padding='post')
  print(" {:5.2f}%\n".format(100*(model.predict(text_padded)[0][0])))

for text in testing_text:
  print(text)
  input(text)

